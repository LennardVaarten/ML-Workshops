{"cells":[{"cell_type":"code","execution_count":null,"id":"8c344b97","metadata":{"id":"8c344b97"},"outputs":[],"source":["# importing libraries, etc...\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","sns.set()\n","\n","path = \"https://raw.githubusercontent.com/LennardVaarten/ML-Workshops/main/data/\""]},{"cell_type":"markdown","id":"62c340ff","metadata":{"id":"62c340ff"},"source":["The mnist dataset contains 70,000 labeled handwritten digits. The digits were recorded as 28x28 images, and the dataset contains the grey values of all the pixel values (so 28*28=784 pixel values for each sample). The aim is to create a model that can accurately classify whether a given digit is a 0, 1, 2, etc... \n","\n","Examples:"]},{"cell_type":"markdown","id":"2dd9b707","metadata":{"id":"2dd9b707"},"source":["<div>\n","<img src=\"https://raw.githubusercontent.com/LennardVaarten/ML-Workshops/main/media/mnist.png\" width=\"500\"/>\n","</div>"]},{"cell_type":"markdown","id":"3cc60eff","metadata":{"id":"3cc60eff"},"source":["To make model-building a little more feasible, I've taken a subset of the dataset containing 20,000 samples, rather than the full 70,000. "]},{"cell_type":"code","execution_count":null,"id":"a0e778a8","metadata":{"scrolled":true,"id":"a0e778a8"},"outputs":[],"source":["# loading\n","\n","mnist = pd.read_csv(path+\"mnist.csv\")"]},{"cell_type":"code","source":["# viewing\n","\n","mnist"],"metadata":{"id":"XwTGRFfeNRXU"},"id":"XwTGRFfeNRXU","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8aca365d","metadata":{"id":"8aca365d"},"outputs":[],"source":["# Check if there are any missing values...\n","\n","mnist.isna().any(axis=1).sum()\n","\n","# Nope!"]},{"cell_type":"markdown","id":"bee757b2","metadata":{"id":"bee757b2"},"source":["Splitting into training and test set. \n","\n","(Changing the random_state parameter in the train_test_split function to a different number will result in a different random split of the data. Try playing around with it and then running your model(s) again to see how a different split might result in a different score.)"]},{"cell_type":"code","execution_count":null,"id":"45b22ecf","metadata":{"id":"45b22ecf"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","features_train_orig, features_test_orig, target_train, target_test = train_test_split(mnist.iloc[:,1:], \n","                                                                    mnist.iloc[:,0], \n","                                                                    random_state=99)"]},{"cell_type":"markdown","id":"fef1fbbd","metadata":{"id":"fef1fbbd"},"source":["Let's start with our good old k-NN classifier...\n","\n","Note that even this subset I've taken of the mnist dataset is quite large, with 20,000 samples and 784 features. Because of this, model-building / prediction can take some time.\n","\n","However, we can save a bit of time by using the n_jobs parameter when building a model. Setting n_jobs=-1 (as done below) tells sklearn to use all of your CPU cores, whereas by default it will only use 1. This means that if your PC has 8 CPU cores, building the model will be 8 times as fast!"]},{"cell_type":"code","execution_count":null,"id":"87cbbcad","metadata":{"id":"87cbbcad"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier(n_neighbors=9, weights=\"uniform\", n_jobs=-1).fit(features_train_orig, target_train)\n","print(\"Training set score: {:.4f}\".format(knn.score(features_train_orig, target_train)))\n","print(\"Test set score: {:.4f}\".format(knn.score(features_test_orig, target_test)))"]},{"cell_type":"markdown","id":"793286b2","metadata":{"id":"793286b2"},"source":["Now, it's your turn to use any of the models we've discussed to see how well they perform on this task. Note that this is a classification problem, so only classification models will work on it. Perhaps even more important than choosing a classifier is trying out different parameter settings (e.g. n_neighbors for k-Nearest Neighbors, C for Logistic Regression, n_estimators for the Random Forest Classifier, etc...). \n","\n","Below are the classification models we've discussed, along with the import statement and the parameters that we've covered during the sessions.\n","\n","- **k-Nearest Neighbors Classifier** (already imported in the cell above)\n","    - n_neighbors (any number above 0)\n","    - weights (\"uniform\", \"distance\")\n","- **Linear Support Vector Machine** (from sklearn.svm import LinearSVC)\n","    - C\n","- **Decision Tree Classifier** (from sklearn.tree import DecisionTreeClassifier)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Random Forest Classifier** (from sklearn.ensemble import RandomForestClassifier)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Gradient Boosting Classifier** (from sklearn.ensemble import GradientBoostingClassifier)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","    - learning_rate (a number between 0 and 1)\n","    - subsample (a number between 0 and 1)\n","    \n","If you want to access even more parameter settings than we've discussed in class (models tend to have a lot), you can also access the sklearn documentation. For example, [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), you can find all possible parameters to tune for the KNeighborsClassifier.\n","\n","Good luck and feel free to share your model (and the results you obtain with it) on the Canvas discussion page!"]},{"cell_type":"code","execution_count":null,"id":"67eb4f17","metadata":{"id":"67eb4f17"},"outputs":[],"source":["# Check if dataset is severely imbalanced\n","\n","target_train.value_counts()"]},{"cell_type":"code","execution_count":null,"id":"3484a848","metadata":{"id":"3484a848"},"outputs":[],"source":["# A quick look at the dataset showed me there are a lot of 0 values; let's check if there are any columns that only contain \n","# the value 0 (i.e., pixels that are always black)\n","\n","blackPixels = [col for col in mnist.columns if len(mnist[col].value_counts()) == 1]\n","\n","print(len(blackPixels))"]},{"cell_type":"code","execution_count":null,"id":"e622cf20","metadata":{"id":"e622cf20"},"outputs":[],"source":["# Drop columns that represent always-black pixels\n","\n","mnistDrop = mnist.drop(columns=blackPixels)"]},{"cell_type":"code","execution_count":null,"id":"bf03bd3d","metadata":{"id":"bf03bd3d"},"outputs":[],"source":["mnistDrop.shape"]},{"cell_type":"code","execution_count":null,"id":"00e07429","metadata":{"id":"00e07429"},"outputs":[],"source":["features_train, features_test, target_train, target_test = train_test_split(mnistDrop.iloc[:,1:], \n","                                                    mnistDrop.iloc[:,0], \n","                                                    random_state=99)"]},{"cell_type":"code","execution_count":null,"id":"92e83597","metadata":{"id":"92e83597"},"outputs":[],"source":["knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-1).fit(features_train, target_train)\n","print(\"Training set score: {:.4f}\".format(knn.score(features_train, target_train)))\n","print(\"Test set score: {:.4f}\".format(knn.score(features_test, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"06e127a0","metadata":{"id":"06e127a0"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=50).fit(features_train)"]},{"cell_type":"code","execution_count":null,"id":"07b6a8ec","metadata":{"id":"07b6a8ec"},"outputs":[],"source":["features_train_pca = pca.transform(features_train)\n","features_test_pca = pca.transform(features_test)"]},{"cell_type":"code","execution_count":null,"id":"1dae01a3","metadata":{"collapsed":true,"id":"1dae01a3"},"outputs":[],"source":["features_train_pca.shape"]},{"cell_type":"code","execution_count":null,"id":"9ab03fd8","metadata":{"id":"9ab03fd8"},"outputs":[],"source":["pca.explained_variance_ratio_"]},{"cell_type":"code","execution_count":null,"id":"26886530","metadata":{"scrolled":true,"id":"26886530"},"outputs":[],"source":["sum(pca.explained_variance_ratio_)"]},{"cell_type":"code","execution_count":null,"id":"c1c291e2","metadata":{"scrolled":true,"id":"c1c291e2"},"outputs":[],"source":["knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-1).fit(features_train_pca, target_train)\n","print(\"Training set score: {:.4f}\".format(knn.score(features_train_pca, target_train)))\n","print(\"Test set score: {:.4f}\".format(knn.score(features_test_pca, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"1c2bf45a","metadata":{"id":"1c2bf45a"},"outputs":[],"source":["# Visualizing the 'eigendigits'\n","\n","pca_orig = PCA(n_components=50).fit(features_train_orig)\n","\n","pca_orig.components_\n","\n","fig, axes = plt.subplots(5, 2, figsize=(6,12))\n","\n","for i in range(10):\n","    axes[i//2][i%2].imshow(np.reshape(pca_orig.components_[i,:], (28,28)), cmap=\"Greys\")\n","    plt.subplots_adjust(wspace=None, hspace=None)"]},{"cell_type":"code","source":["# Let's just take one random digit...\n","\n","plt.imshow(features_train_orig.iloc[0:1,:].values.reshape((28,28)), cmap=\"Greys_r\")"],"metadata":{"id":"kO-zMx4eXnKX"},"id":"kO-zMx4eXnKX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How would our PCA 'encode' this digit in 50 components?\n","\n","examplePCA = pca_orig.transform(features_train_orig.iloc[0:1,:])\n","\n","fig, axes = plt.subplots(25, 2, figsize=(10,85))\n","\n","for i in range(50):\n","    ax = axes[i//2][i%2]\n","    ax.title.set_text(examplePCA[0][i])\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    axes[i//2][i%2].imshow(np.reshape(pca_orig.components_[i,:], (28,28)), cmap=\"Greys\")\n","    plt.subplots_adjust(wspace=None, hspace=None)"],"metadata":{"id":"rKPBzopfSjzi"},"id":"rKPBzopfSjzi","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b8f7ff28","metadata":{"id":"b8f7ff28"},"outputs":[],"source":["# let's zoom in on our model's misclassifications...\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","target_pred = knn.predict(features_test_pca)\n","confusion = confusion_matrix(target_test, target_pred)\n","\n","disp = ConfusionMatrixDisplay(confusion_matrix=confusion, display_labels=[x for x in range(10)])\n","\n","disp.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"9ae44ed0","metadata":{"id":"9ae44ed0"},"outputs":[],"source":["fours_as_nines = features_test_orig[(target_test==4) & (target_pred==9)].reset_index(drop=True)\n","\n","fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20,20))\n","\n","for index, row in fours_as_nines.iterrows():\n","    axes[index//5][index%5].imshow(row.values.reshape((28,28)), cmap=\"Greys_r\")\n","    \n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"0abe04e0","metadata":{"id":"0abe04e0"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=3, subsample=1, learning_rate=0.1, random_state=99, n_jobs=-1, verbose=1)\n","gbc.fit(features_train_pca, target_train)\n","\n","print(\"Accuracy on training set: {:.4f}\".format(gbc.score(features_train_pca, target_train)))\n","print(\"Accuracy on test set: {:.4f}\".format(gbc.score(features_test_pca, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"1ed0abb2","metadata":{"id":"1ed0abb2"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rfr = RandomForestClassifier(n_estimators=1000, random_state=99, n_jobs=-1, verbose=1)\n","rfr.fit(features_train_pca, target_train)\n","\n","print(\"Accuracy on training set: {:.4f}\".format(rfr.score(features_train_pca, target_train)))\n","print(\"Accuracy on test set: {:.4f}\".format(rfr.score(features_test_pca, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"eb086a3d","metadata":{"id":"eb086a3d"},"outputs":[],"source":["rfr.feature_importances_"]},{"cell_type":"code","execution_count":null,"id":"8cac4692","metadata":{"id":"8cac4692"},"outputs":[],"source":["mnistDrop[\"total\"] = mnistDrop.iloc[:,1:].sum(axis=1)\n","\n","mnistDrop[\"total\"].idxmin()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"CLASS_mnist.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}