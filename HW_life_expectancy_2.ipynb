{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW_life_expectancy_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2dB8CIisx3MN6LrLYVjV7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f9a82843"},"outputs":[],"source":["# importing libraries, etc...\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","path = \"https://raw.githubusercontent.com/LennardVaarten/ML-Workshops/main/data/\""]},{"cell_type":"markdown","metadata":{"id":"399b6221"},"source":["The [Gapminder](https://www.gapminder.org/) dataset contains historical data (mid-19th century onwards) containing hundreds of indicators such as life expectancy and GDP for countries around the world.\n","For our purpose, we will try to predict the life expectancy of countries based on several of these indicators. I have only included data from the year 2018."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d4328f5"},"outputs":[],"source":["life_expectancy = pd.read_csv(path+\"life_expectancy.csv\")\n","\n","life_expectancy"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4ebb8645"},"outputs":[],"source":["# checking the number of missing values per feature\n","\n","life_expectancy.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"572e062a"},"outputs":[],"source":["# Imputing missing values using the k-NN algorithm, with n_neighbors=3\n","\n","from sklearn.impute import KNNImputer\n","\n","imputer = KNNImputer(n_neighbors=3).fit(life_expectancy.iloc[:,:-1])\n","life_expectancy.iloc[:,:-1] = imputer.transform(life_expectancy.iloc[:,:-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad5aabd5"},"outputs":[],"source":["# voila: no more missing values!\n","\n","life_expectancy.isna().sum()"]},{"cell_type":"code","source":["life_expectancy"],"metadata":{"id":"F7NjSzrMBrhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scaling\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler().fit(life_expectancy.iloc[:,1:-1])\n","life_expectancy.iloc[:,1:-1] = scaler.transform(life_expectancy.iloc[:, 1:-1])"],"metadata":{"id":"fKzgVcsZBoBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"554c2c89"},"outputs":[],"source":["# splitting into training and test set\n","\n","from sklearn.model_selection import train_test_split\n","\n","features_train, features_test, target_train, target_test = train_test_split(life_expectancy.iloc[:,1:-1],\n","                                                                                       life_expectancy.iloc[:,0],\n","                                                                                       test_size=0.35,\n","                                                                                       random_state=99)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"0116d3db"},"outputs":[],"source":["train = pd.concat([target_train, features_train], axis=1)\n","\n","fig, axes = plt.subplots(3,3, figsize=(18,16))\n","\n","for i in range(len(train.columns)-1):\n","    sns.scatterplot(data=train, ax=axes[i//3, i%3], x=train.columns[i+1], y=train.columns[0])\n","\n","fig.tight_layout(pad=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"649b9ecb"},"outputs":[],"source":["# example using Grid Search and Cross Validation to find the optimal parameters. Here, I have used 10 folds, but feel free to use \n","# more or fewer in the model(s) you make below!\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","params = {\n","    \"n_neighbors\": [1, 3, 5, 7, 9, 11],\n","    \"weights\": [\"uniform\", \"distance\"]\n","}\n","\n","knn = GridSearchCV(estimator=KNeighborsRegressor(),\n","                   param_grid=params, cv=10) \n","\n","knn.fit(features_train, target_train)\n","\n","print(\"Training set score: {:.4f}\".format(knn.score(features_train, target_train)))\n","print(\"Test set score: {:.4f}\".format(knn.score(features_test, target_test)))\n","print(knn.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"742135b7"},"source":["Now, it's your turn to use any of the models we've discussed to see how well they perform on this task. Since this dataset is significantly smaller than the mnist (handwritten digits) dataset, it is very feasible - and, practically a requirement - to use Grid Search and Cross Validation to build and test your models. Note that this is a regression problem and classification models will thus not work on it. Perhaps even more important than choosing a classifier is trying out different parameter settings (e.g. n_neighbors and weights for k-Nearest Neighbors). \n","\n","Below are the regression models we've discussed, along with the import statement and the parameters that we've covered during the sessions.\n","\n","- **k-Nearest Neighbors Regression** (already imported in the cell above)\n","    - n_neighbors (any number above 0)\n","    - weights (\"uniform\", \"distance\")\n","- **Linear Regression** (from sklearn.linear_model import LinearRegression)\n","    - No parameters to tune\n","- **Ridge Regression** (from sklearn.linear_model import Ridge)\n","    - alpha (any number above 0)\n","- **Lasso Regression** (from sklearn.linear_model import Lasso)\n","    - alpha (any number above 0)\n","- **Decision Tree Regression** (from sklearn.tree import DecisionTreeRegressor)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Random Forest Regression** (from sklearn.ensemble import RandomForestRegressor)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Gradient Boosting Regressor** (from sklearn.ensemble import GradientBoostingRegressor)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","    - learning_rate (a number between 0 and 1)\n","    - subsample (a number between 0 and 1)\n","    \n","If you want to access even more parameter settings than we've discussed in class (models tend to have a lot), you can also access the sklearn documentation. For example, [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), you can find all possible parameters to tune for the KNeighborsClassifier.\n","\n","Good luck and feel free to share your model (and the results you obtain with it) on the Canvas discussion page!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6799b2cc"},"outputs":[],"source":[""]}]}