{"cells":[{"cell_type":"code","execution_count":null,"id":"f9a82843","metadata":{"id":"f9a82843"},"outputs":[],"source":["# importing libraries, etc...\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","path = \"https://raw.githubusercontent.com/LennardVaarten/ML-Workshops/main/data/\""]},{"cell_type":"markdown","id":"399b6221","metadata":{"id":"399b6221"},"source":["The [Gapminder](https://www.gapminder.org/) dataset contains historical data (mid-19th century onwards) containing hundreds of indicators such as life expectancy and GDP for countries around the world.\n","For our purpose, we will try to predict the life expectancy of countries based on several of these indicators.\n","\n","To make experimenting with Cross-Validation and Grid Search on the life_expectancy dataset a bit more feasible, I have only included data from the year 2018. I have titled this subset of the life_expectancy dataset life_expectancy.csv."]},{"cell_type":"code","execution_count":null,"id":"9d4328f5","metadata":{"id":"9d4328f5"},"outputs":[],"source":["# loading the data\n","\n","life_expectancy = pd.read_csv(path+\"life_expectancy.csv\")"]},{"cell_type":"code","source":["# viewing the data\n","\n","life_expectancy"],"metadata":{"id":"r6PozEpdQu03"},"id":"r6PozEpdQu03","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4ebb8645","metadata":{"collapsed":true,"id":"4ebb8645"},"outputs":[],"source":["# checking the number of missing values per feature\n","\n","life_expectancy.isna().sum()"]},{"cell_type":"code","execution_count":null,"id":"572e062a","metadata":{"id":"572e062a"},"outputs":[],"source":["# imputing missing values using the k-NN algorithm, with n_neighbors=3\n","\n","from sklearn.impute import KNNImputer\n","\n","imputer = KNNImputer(n_neighbors=3).fit(life_expectancy.iloc[:,:-1])\n","life_expectancy.iloc[:,:-1] = imputer.transform(life_expectancy.iloc[:,:-1])"]},{"cell_type":"code","execution_count":null,"id":"ad5aabd5","metadata":{"id":"ad5aabd5"},"outputs":[],"source":["# voila: no more missing values!\n","\n","life_expectancy.isna().sum()"]},{"cell_type":"code","source":["life_expectancy"],"metadata":{"id":"F7NjSzrMBrhq"},"id":"F7NjSzrMBrhq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scaling\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler().fit(life_expectancy.iloc[:,1:-1])\n","life_expectancy.iloc[:,1:-1] = scaler.transform(life_expectancy.iloc[:, 1:-1])"],"metadata":{"id":"fKzgVcsZBoBF"},"id":"fKzgVcsZBoBF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"554c2c89","metadata":{"id":"554c2c89"},"outputs":[],"source":["# splitting into training and test set\n","\n","from sklearn.model_selection import train_test_split\n","\n","features_train, features_test, target_train, target_test = train_test_split(life_expectancy.iloc[:,1:-1],\n","                                                                                       life_expectancy.iloc[:,0],\n","                                                                                       test_size=0.35,\n","                                                                                       random_state=99)"]},{"cell_type":"code","execution_count":null,"id":"0116d3db","metadata":{"scrolled":true,"id":"0116d3db"},"outputs":[],"source":["train = pd.concat([target_train, features_train], axis=1)\n","\n","fig, axes = plt.subplots(3,3, figsize=(18,16))\n","\n","for i in range(len(train.columns)-1):\n","    sns.scatterplot(data=train, ax=axes[i//3, i%3], x=train.columns[i+1], y=train.columns[0])\n","\n","fig.tight_layout(pad=2)"]},{"cell_type":"code","execution_count":null,"id":"649b9ecb","metadata":{"id":"649b9ecb"},"outputs":[],"source":["# using Grid Search and Cross Validation to find the optimal parameters. Here, I have used 10 folds, but feel free to use \n","# more or fewer in the model(s) you make below!\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","params = {\n","    \"n_neighbors\": [1, 3, 5, 7, 9, 11],\n","    \"weights\": [\"uniform\", \"distance\"]\n","}\n","\n","knn = GridSearchCV(estimator=KNeighborsRegressor(),\n","                   param_grid=params, cv=10) \n","\n","knn.fit(features_train, target_train)\n","\n","print(\"Training set score: {:.4f}\".format(knn.score(features_train, target_train)))\n","print(\"Test set score: {:.4f}\".format(knn.score(features_test, target_test)))\n","print(knn.best_params_)"]},{"cell_type":"markdown","id":"742135b7","metadata":{"id":"742135b7"},"source":["Now, it's your turn to use any of the models we've discussed to see how well they perform on this task. Since this dataset is significantly smaller than the mnist (handwritten digits) dataset, it is very feasible - and, practically a requirement - to use Grid Search and Cross Validation to build and test your models. Note that this is a regression problem and classification models will thus not work on it. Perhaps even more important than choosing a classifier is trying out different parameter settings (e.g. n_neighbors for k-Nearest Neighbors, C for Logistic Regression, n_estimators for the Random Forest Classifier, etc...). \n","\n","Below are the regression models we've discussed, along with the import statement and the parameters that we've covered during the sessions.\n","\n","- **k-Nearest Neighbors Regression** (already imported in the cell above)\n","    - n_neighbors (any number above 0)\n","    - weights (\"uniform\", \"distance\")\n","- **Linear Regression** (from sklearn.linear_model import LinearRegression)\n","    - C\n","- **Ridge Regression** (from sklearn.linear_model import Ridge)\n","    - alpha (any number above 0)\n","- **Lasso Regression** (from sklearn.linear_model import Lasso)\n","    - alpha (any number above 0)\n","- **Decision Tree Regression** (from sklearn.tree import DecisionTreeRegressor)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Random Forest Regression** (from sklearn.ensemble import RandomForestRegressor)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","- **Gradient Boosting Regressor** (from sklearn.ensemble import GradientBoostingRegressor)\n","    - n_estimators (a whole number above 0)\n","    - max_depth (a whole number above 0)\n","    - min_samples_split (a whole number above 1)\n","    - learning_rate (a number between 0 and 1)\n","    - subsample (a number between 0 and 1)\n","    \n","If you want to access even more parameter settings than we've discussed in class (models tend to have a lot), you can also access the sklearn documentation. For example, [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), you can find all possible parameters to tune for the KNeighborsClassifier.\n","\n","Good luck and feel free to share your model(s) (and the results you obtain with it) on the Canvas discussion page!"]},{"cell_type":"code","execution_count":null,"id":"6799b2cc","metadata":{"id":"6799b2cc"},"outputs":[],"source":["# Example without grid search: Linear Regression\n","\n","from sklearn.linear_model import LinearRegression\n","\n","lr = LinearRegression().fit(features_train, target_train)\n","\n","print(\"Training set score: {:.4f}\".format(lr.score(features_train, target_train)))\n","print(\"Test set score: {:.4f}\".format(lr.score(features_test, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"ecb4956e","metadata":{"id":"ecb4956e"},"outputs":[],"source":["# Now let's go one step further and try Gradient Boosting, with parameters optimized using Grid Search + Cross-Validation\n","\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","params = {\n","    \"n_estimators\": [500],\n","    \"learning_rate\": [0.01, 0.05, 0.1, 0.15],\n","    \"max_depth\": [3, 6, None],\n","}\n","\n","gbr = GridSearchCV(estimator=GradientBoostingRegressor(),\n","                   param_grid=params, cv=5, verbose=1) \n","\n","gbr.fit(features_train, target_train)\n","\n","print(gbr.best_params_)\n","print(\"Training set score: {:.4f}\".format(gbr.best_score_))\n","print(\"Test set score: {:.4f}\".format(gbr.score(features_test, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"5f371b5e","metadata":{"id":"5f371b5e"},"outputs":[],"source":["# To do even better, we might have to get a little more creative. \n","# Let's go back to using linear models, but after adding polynomial features and interaction terms.\n","\n","life_expectancy = pd.read_csv(path+\"life_expectancy.csv\")\n","imputer = KNNImputer(n_neighbors=3).fit(life_expectancy.iloc[:,:-1])\n","life_expectancy.iloc[:,:-1] = imputer.transform(life_expectancy.iloc[:,:-1])\n","\n","le_poly = pd.DataFrame([life_expectancy.country]).T\n","colsDone = set()\n","\n","for col in life_expectancy.columns[1:-1]:\n","    le_poly.insert(0, col, life_expectancy[col])\n","    le_poly.insert(0, f\"{col}**2\", life_expectancy[col]**2)\n","    for col2 in life_expectancy.iloc[:,1:-1].columns:\n","        if col2 in colsDone:\n","          continue\n","        elif col != col2:\n","            le_poly.insert(0, f\"{col} * {col2}\", life_expectancy[col] * life_expectancy[col2])\n","    colsDone.add(col)\n","\n","le_poly.insert(0, \"life_expectancy_years\", life_expectancy[\"life_expectancy_years\"])\n","\n","scaler = MinMaxScaler().fit(le_poly.iloc[:, 1:-1])\n","le_poly.iloc[:,1:-1] = scaler.transform(le_poly.iloc[:, 1:-1])"]},{"cell_type":"code","execution_count":null,"id":"95426204","metadata":{"id":"95426204"},"outputs":[],"source":["le_poly"]},{"cell_type":"code","execution_count":null,"id":"917ac26e","metadata":{"id":"917ac26e"},"outputs":[],"source":["# splitting\n","\n","poly_features_train, poly_features_test, poly_target_train, poly_target_test = train_test_split(le_poly.iloc[:,1:-1],\n","                                                                                       le_poly.iloc[:,0],\n","                                                                                       test_size=0.35,\n","                                                                                       random_state=99)"]},{"cell_type":"code","execution_count":null,"id":"05bce37b","metadata":{"id":"05bce37b"},"outputs":[],"source":["# linear regression with polynomial features\n","\n","from sklearn.linear_model import LinearRegression\n","\n","lr = LinearRegression().fit(poly_features_train, target_train)\n","\n","print(\"{:.4f}\".format(lr.score(poly_features_test, target_test)))"]},{"cell_type":"code","execution_count":null,"id":"3f79d11f","metadata":{"id":"3f79d11f"},"outputs":[],"source":["# Since we have so many features (and few data points), Lasso might be a good idea...\n","\n","from sklearn.linear_model import Lasso\n","\n","params = {\n","    \"alpha\": [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n","}\n","\n","lasso = GridSearchCV(estimator=Lasso(),\n","                   param_grid=params, cv=10, n_jobs=-1, verbose=1) \n","\n","lasso.fit(poly_features_train, poly_target_train)\n","\n","print(lasso.best_params_)\n","print(\"CV score: {:.4f}\".format(lasso.best_score_))\n","print(\"Test set score: {:.4f}\".format(lasso.score(poly_features_test, poly_target_test)))"]},{"cell_type":"code","execution_count":null,"id":"7ae1dc76","metadata":{"scrolled":true,"id":"7ae1dc76"},"outputs":[],"source":["# how many features were used?\n","\n","print(\"Total features used: {}\".format(len(list(poly_features_train.columns[(lasso.best_estimator_.coef_ != 0).ravel()]))))\n","print(\"Total features discarded: {}\".format(len(list(poly_features_train.columns[(lasso.best_estimator_.coef_ == 0).ravel()]))))"]},{"cell_type":"code","source":["# what features were used?\n","\n","print(\"Features used:\")\n","\n","for i in poly_features_train.columns[(lasso.best_estimator_.coef_ != 0).ravel()]:\n","  print(i)"],"metadata":{"id":"hFeq5TZaFyPt"},"id":"hFeq5TZaFyPt","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a78be373","metadata":{"id":"a78be373"},"outputs":[],"source":["# what did our model predict?\n","\n","y_pred_y = pd.DataFrame()\n","\n","y_pred_y[\"y\"] = poly_target_test\n","y_pred_y[\"y_pred\"] = lasso.predict(poly_features_test)\n","y_pred_y.insert(0, \"country\", [life_expectancy.loc[i, \"country\"] for i in y_pred_y.index])\n","\n","y_pred_y"]},{"cell_type":"code","execution_count":null,"id":"165127d0","metadata":{"scrolled":true,"id":"165127d0"},"outputs":[],"source":["# just for fun, let's do a manual calculation of R2\n","\n","y_pred_y[\"residuals\"] = (y_pred_y[\"y\"] - y_pred_y[\"y_pred\"])\n","y_pred_y[\"squared_residuals\"] = (y_pred_y[\"residuals\"])**2\n","\n","y_pred_y"]},{"cell_type":"code","execution_count":null,"id":"7caa0a51","metadata":{"id":"7caa0a51"},"outputs":[],"source":["y_pred_y[\"mean_target_test\"] = np.repeat(np.mean(y_pred_y[\"y\"]), y_pred_y.shape[0])\n","\n","y_pred_y"]},{"cell_type":"code","execution_count":null,"id":"597caa8b","metadata":{"id":"597caa8b"},"outputs":[],"source":["y_pred_y[\"squared_residuals_from_mean\"] = (y_pred_y[\"y\"] - y_pred_y[\"mean_target_test\"])**2\n","\n","y_pred_y"]},{"cell_type":"code","execution_count":null,"id":"11d905d2","metadata":{"id":"11d905d2"},"outputs":[],"source":["print(1 - (sum(y_pred_y[\"squared_residuals\"]) / sum(y_pred_y[\"total_squared_residuals\"])))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"CLASS_life_expectancy.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}